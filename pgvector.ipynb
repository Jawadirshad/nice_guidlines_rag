{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2-binary pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU langchain_postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "connection = \"postgresql+psycopg://postgres:test@localhost:5432/vector_db\"  # Uses psycopg3!\n",
    "collection_name = \"my_docs\"\n",
    "\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract text from Markdown files and find the first link\n",
    "def extract_text_from_markdown(md_path):\n",
    "    text = \"\"\n",
    "    pdf_link = None\n",
    "    try:\n",
    "        with open(md_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            # Find the first link in the Markdown text using regex\n",
    "            match = re.search(r'\\b(www\\.nice\\.org\\.uk/guidance/\\S*)\\b', text)  # Adjusted regex for specific link format\n",
    "            if match:\n",
    "                pdf_link = match.group(1)  # Extract the URL from the match\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract text from {md_path}: {e}\")\n",
    "    return text, pdf_link\n",
    "\n",
    "# Step 2: Chunk text into smaller pieces with semantic chunking and overlapping\n",
    "def chunk_text(text, chunk_size=3000, overlap_ratio=0.3):\n",
    "    sentences = nltk.sent_tokenize(text)  # Split text into sentences\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    overlap_size = int(chunk_size * overlap_ratio)  # Calculate overlap size in terms of word count\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        sentence = sentences[i]\n",
    "        sentence_length = len(sentence.split())\n",
    "\n",
    "        # If adding the sentence exceeds the chunk size, finalize the current chunk\n",
    "        if current_length + sentence_length > chunk_size and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            # Calculate the number of sentences to overlap (based on overlap_size)\n",
    "            overlap_sentences = 0\n",
    "            overlap_length = 0\n",
    "            for s in reversed(current_chunk):\n",
    "                s_length = len(s.split())\n",
    "                if overlap_length + s_length > overlap_size:\n",
    "                    break\n",
    "                overlap_sentences += 1\n",
    "                overlap_length += s_length\n",
    "            # Start a new chunk with the last `overlap_sentences` sentences of the previous chunk\n",
    "            current_chunk = current_chunk[-overlap_sentences:]\n",
    "            current_length = sum(len(s.split()) for s in current_chunk)\n",
    "        else:\n",
    "            # Add the sentence to the current chunk\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "            i += 1\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Step 3: Create vector store with pgvector\n",
    "def create_vector_store(md_folder, db_config, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Fetch all Markdown files in the folder\n",
    "    md_files = glob.glob(os.path.join(md_folder, \"*.md\"))\n",
    "\n",
    "    if not md_files:\n",
    "        print(f\"No Markdown files found in the folder: {md_folder}\")\n",
    "        return\n",
    "\n",
    "    print(\"Extracting text and generating embeddings...\")\n",
    "    text_chunks = []\n",
    "    metadata = []\n",
    "\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    register_vector(conn)  # Register pgvector extension\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create table for embeddings if it doesn't exist\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS document_embeddings (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            md_file TEXT,\n",
    "            pdf_link TEXT,\n",
    "            chunk_index INT,\n",
    "            chunk_text TEXT,\n",
    "            embedding vector(384)  -- Adjust dimension based on your model\n",
    "        );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "\n",
    "    for md_file in tqdm(md_files, desc=\"Processing Markdown files\"):\n",
    "        print(f\"Processing file: {md_file}\")\n",
    "\n",
    "        text, pdf_link = extract_text_from_markdown(md_file)\n",
    "        if not text.strip():\n",
    "            print(f\"Warning: No text extracted from {md_file}\")\n",
    "            continue\n",
    "\n",
    "        chunks = chunk_text(text)\n",
    "        if not chunks:\n",
    "            print(f\"Warning: No valid chunks created for {md_file}\")\n",
    "            continue\n",
    "\n",
    "        embeddings = model.encode(chunks)\n",
    "        if embeddings.size == 0:\n",
    "            print(f\"Warning: No embeddings generated for {md_file}\")\n",
    "            continue\n",
    "\n",
    "        # Insert embeddings and metadata into PostgreSQL\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO document_embeddings (md_file, pdf_link, chunk_index, chunk_text, embedding)\n",
    "                VALUES (%s, %s, %s, %s, %s);\n",
    "            \"\"\", (md_file, pdf_link, i, chunk, embedding.tolist()))\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(\"Vector store created successfully in PostgreSQL with pgvector!\")\n",
    "\n",
    "# Main pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Change directory paths for Markdown files\n",
    "    md_folder = \"/Users/umer/Desktop/rag_nice/pdf_data_markdown_cleaned\"  # Folder containing Markdown files\n",
    "\n",
    "    # PostgreSQL connection configuration\n",
    "    db_config = {\n",
    "        \"dbname\": \"vector_db\",\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"test\",\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 5432\n",
    "    }\n",
    "\n",
    "    # Ensure Markdown folder exists\n",
    "    if not os.path.exists(md_folder):\n",
    "        raise ValueError(f\"Markdown folder not found at {md_folder}. Please upload the folder first.\")\n",
    "\n",
    "    # Create the vector store\n",
    "    create_vector_store(md_folder, db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is risk assesement in antrnatal care?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import json\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "from groq import Groq\n",
    "import streamlit as st\n",
    "\n",
    "# Step 4: Query the vector store using pgvector\n",
    "def query_vector_store(query, db_config, top_k=3, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    register_vector(conn)  # Register pgvector extension\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Encode the query into an embedding\n",
    "    query_embedding = model.encode([query])\n",
    "\n",
    "    # Query the database for the top-k most similar embeddings\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT md_file, pdf_link, chunk_index, chunk_text\n",
    "        FROM document_embeddings\n",
    "        ORDER BY embedding <-> %s\n",
    "        LIMIT %s;\n",
    "    \"\"\", (query_embedding.tolist(), top_k))\n",
    "\n",
    "    results = []\n",
    "    for row in cur.fetchall():\n",
    "        md_file, pdf_link, chunk_index, chunk_text = row\n",
    "        results.append({\n",
    "            \"md_file\": md_file,\n",
    "            \"pdf_link\": pdf_link,\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"chunk_text\": chunk_text\n",
    "        })\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Step 5: Generate an answer using Groq\n",
    "def generate_answer_with_groq(query, retrieved_texts, groq_api_key):\n",
    "    client = Groq(api_key=groq_api_key)\n",
    "\n",
    "    # Construct the prompt using the retrieved context\n",
    "    prompt = (\n",
    "        f\"The following texts were retrieved as context:\\n\\n\"\n",
    "        f\"{retrieved_texts}\\n\\n\"\n",
    "        f\"Based on the context, answer the query:\\n\\n\"\n",
    "        f\"{query}\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.1,\n",
    "        model=\"llama-3.1-8b-instant\",  # Replace with your desired Groq model\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Streamlit UI\n",
    "def main():\n",
    "    st.title(\"AI ASSISTANT FOR NICE GUIDELINES\")\n",
    "    st.write(\"Welcome to the NICE Guidelines assistance\")\n",
    "\n",
    "    # Create session state for maintaining query history\n",
    "    if 'history' not in st.session_state:\n",
    "        st.session_state.history = []\n",
    "\n",
    "    # Inputs: query input field and submit button\n",
    "    query = st.text_input(\"Enter your query to get assistance:\")\n",
    "\n",
    "    if st.button(\"Submit\"):\n",
    "        if query:\n",
    "            # PostgreSQL connection configuration\n",
    "            db_config = {\n",
    "                \"dbname\": \"vector_db\",\n",
    "                \"user\": \"postgres\",\n",
    "                \"password\": \"test\",\n",
    "                \"host\": \"localhost\",\n",
    "                \"port\": 5432\n",
    "            }\n",
    "\n",
    "            groq_api_key = \"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\"  # Replace with your Groq API key\n",
    "\n",
    "            # Start timer for retrieval\n",
    "            retrieval_start_time = time.time()\n",
    "\n",
    "            # Retrieve context using the vector store\n",
    "            with st.spinner(\"Retrieving relevant documents...\"):\n",
    "                results = query_vector_store(query, db_config)\n",
    "                retrieved_texts = \"\\n\\n\".join([res[\"chunk_text\"] for res in results])\n",
    "                pdf_links = [res[\"pdf_link\"] for res in results]\n",
    "\n",
    "            # Calculate retrieval time\n",
    "            retrieval_elapsed_time = time.time() - retrieval_start_time\n",
    "\n",
    "            # Show retrieved texts to the user\n",
    "            st.subheader(\"Retrieved Context:\")\n",
    "            st.text_area(\"Context\", retrieved_texts, height=300)\n",
    "\n",
    "            # Start timer for answer generation\n",
    "            generation_start_time = time.time()\n",
    "\n",
    "            # Generate an answer using Groq\n",
    "            with st.spinner(\"Generating answer...\"):\n",
    "                answer = generate_answer_with_groq(query, retrieved_texts, groq_api_key)\n",
    "\n",
    "            # Calculate generation time\n",
    "            generation_elapsed_time = time.time() - generation_start_time\n",
    "\n",
    "            # Display the answer and times\n",
    "            st.subheader(\"Answer:\")\n",
    "            st.write(answer)\n",
    "\n",
    "            st.subheader(\"Elapsed Time:\")\n",
    "            st.write(f\"Time for context retrieval: {retrieval_elapsed_time:.2f} seconds\")\n",
    "            st.write(f\"Time for answer generation: {generation_elapsed_time:.2f} seconds\")\n",
    "            st.write(f\"Total time: {retrieval_elapsed_time + generation_elapsed_time:.2f} seconds\")\n",
    "\n",
    "            # Display references\n",
    "            if pdf_links:\n",
    "                st.subheader(\"References:\")\n",
    "                for link in pdf_links:\n",
    "                    cleaned_link = link.removeprefix(\"http://localhost:8501/\")\n",
    "                    if not cleaned_link.startswith(\"http\"):\n",
    "                        cleaned_link = \"https://\" + cleaned_link\n",
    "                    st.markdown(f\"{cleaned_link}\")\n",
    "\n",
    "            # Add the query and answer to history\n",
    "            st.session_state.history.append({\n",
    "                \"query\": query,\n",
    "                \"answer\": answer,\n",
    "                \"retrieval_time\": retrieval_elapsed_time,\n",
    "                \"generation_time\": generation_elapsed_time,\n",
    "                \"total_time\": retrieval_elapsed_time + generation_elapsed_time\n",
    "            })\n",
    "\n",
    "    # Sidebar for interaction history\n",
    "    st.sidebar.title(\"Interaction History\")\n",
    "    if st.session_state.history:\n",
    "        for _, interaction in enumerate(st.session_state.history):\n",
    "            with st.sidebar.expander(f\"**Question :** {interaction['query']}\"):\n",
    "                st.write(f\"**Answer :** {interaction['answer']}\")\n",
    "                st.write(f\"**Time for Context Retrieval:** {interaction['retrieval_time']:.2f} seconds\")\n",
    "                st.write(f\"**Time for Answer Generation:** {interaction['generation_time']:.2f} seconds\")\n",
    "                st.write(f\"**Total Time:** {interaction['total_time']:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik import Opik\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.metrics import Hallucination, AnswerRelevance\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "from groq import Groq\n",
    "from itertools import islice\n",
    "\n",
    "# Initialize Opik client\n",
    "client = Opik()\n",
    "dataset = client.get_dataset(name=\"ragg_nice\")\n",
    "\n",
    "# limited_dataset = dataset[:2]\n",
    "# limited_dataset = list(dataset)[:2]\n",
    "# limited_dataset = dataset.sample(2)\n",
    "# limited_dataset = list(islice(dataset, 2))\n",
    "\n",
    "# PostgreSQL connection configuration\n",
    "db_config = {\n",
    "    \"dbname\": \"vector_db\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"test\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432\n",
    "}\n",
    "\n",
    "groq_api_key = \"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\"  # Replace with your Groq API key\n",
    "\n",
    "def query_vector_store(query, db_config, top_k=2, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    register_vector(conn)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    query_embedding = model.encode(query).tolist()\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        SELECT md_file, pdf_link, chunk_index, chunk_text\n",
    "        FROM document_embeddings\n",
    "        ORDER BY embedding <-> %s::vector\n",
    "        LIMIT %s;\n",
    "        \"\"\", (query_embedding, top_k)\n",
    "    )\n",
    "\n",
    "    results = [row[3] for row in cur.fetchall()]  # Extracting chunk_text only\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return results\n",
    "\n",
    "def generate_answer_with_groq(query, retrieved_texts, groq_api_key):\n",
    "    client = Groq(api_key=groq_api_key)\n",
    "    prompt = (\n",
    "        f\"The following texts were retrieved as context:\\n\\n\"\n",
    "        f\"{retrieved_texts}\\n\\n\"\n",
    "        f\"Based on the context, answer the query:\\n\\n\"\n",
    "        f\"{query}\"\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=500,\n",
    "        temperature=0.1,\n",
    "        model=\"llama-3.1-8b-instant\"\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def evaluation_task(dataset_item):\n",
    "    query = dataset_item['query']\n",
    "    retrieved_texts = query_vector_store(query, db_config)\n",
    "    answer = generate_answer_with_groq(query, \"\\n\\n\".join(retrieved_texts), groq_api_key)\n",
    "\n",
    "    return {\n",
    "        \"input\": query,\n",
    "        \"output\": answer,\n",
    "        \"context\": retrieved_texts\n",
    "    }\n",
    "\n",
    "metrics = [Hallucination(), AnswerRelevance()]\n",
    "\n",
    "eval_results = evaluate(\n",
    "    experiment_name=\"my_evaluation\",\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=metrics,\n",
    "    task_threads=1,\n",
    ")\n",
    "\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
